# Large Manipulation Model







- [LLM/VLM Guidance](#LLM/VLM-Guidance)

- [Structured Instructions](#Structured-Instructions)
- [Vision-Language-Action](#Vision-Language-Action)



## Video Generation

- **VLP**: Video Language Planning, *arXiv 2023*. [[Paper](https://arxiv.org/abs/2310.10625)] [[Website](https://video-language-planning.github.io/)] [[Code](https://github.com/video-language-planning/vlp_code)]  [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **AVDC**: Learning to Act from Actionless Videos through Dense Correspondences, *arXiv 2023*. [[Paper](https://arxiv.org/abs/2310.08576)] [[Website](https://flow-diffusion.github.io/)] [[Code](https://github.com/flow-diffusion/AVDC)]

- **ATM**: Any-point Trajectory Modeling for Policy Learning, *RSS 2024*. [[Paper](https://arxiv.org/abs/2401.00025)] [[Website](https://xingyu-lin.github.io/atm/)] [[Code](https://github.com/Large-Trajectory-Model/ATM)] [UC Berkeley]
- **Track2Act**: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation, *ECCV 2024*. [[Paper](https://arxiv.org/abs/2405.01527)] [[Website](https://homangab.github.io/track2act/)] [[Code](https://github.com/homangab/Track-2-Act/)] [CMU]
- **Dreamitate**: Real-World Visuomotor Policy Learning via Video Generation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2406.16862)] [[Website](https://dreamitate.cs.columbia.edu/)] [[Code](https://github.com/cvlab-columbia/dreamitate)] [Columbia University]
- **ARDuP**: Active Region Video Diffusion for Universal Policies, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2406.13301)]
- **This&That**: Language-Gesture Controlled Video Generation for Robot Planning, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2407.05530?context=cs)] [[Website](https://cfeng16.github.io/this-and-that/)] [[Code](https://github.com/cfeng16/this-and-that)]
- **Im2Flow2Act**: Flow as the Cross-Domain Manipulation Interface, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2407.15208)] [[Website](https://im-flow-act.github.io/)] [[Code](https://github.com/real-stanford/im2Flow2Act)]  [[REAL-Stanford](https://github.com/real-stanford)]
- **CLOVER**: Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation, *NeurIPS 2024*. [[Paper](https://arxiv.org/abs/2409.09016)] [[Code](https://github.com/OpenDriveLab/CLOVER)] [[OpenDriveLab](https://github.com/OpenDriveLab)]
- **Gen2Act**: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2409.16283)] [[Website](https://homangab.github.io/gen2act/)] [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **DynaMo**: In-Domain Dynamics Pretraining for Visuo-Motor Control, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2409.12192)] [[Website](https://dynamo-ssl.github.io/)] [[Code](https://github.com/jeffacce/dynamo_ssl)]
- **GR-2**: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.06158)] [[Website](https://gr2-manipulation.github.io/)] [Robotics Research Team, ByteDance Research]
- **VLM See, Robot Do**: Human Demo Video to Robot Action Plan via Vision Language Model, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.08792)] [[Website](https://ai4ce.github.io/SeeDo/)] [[Code](https://github.com/ai4ce/SeeDo)]
- Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.08001)] [[Website](https://opendrivelab.com/RoboDual/)]
- **LAPA**: Latent Action Pretraining from Videos, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.11758)] [[Website](https://latentactionpretraining.github.io/)] [[Code](https://github.com/LatentActionPretraining/LAPA)]
- Differentiable Robot Rendering, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.13851)] [[Website](https://drrobot.cs.columbia.edu/)] [[Code](https://github.com/cvlab-columbia/drrobot)] [[cvlab-columbia](https://github.com/cvlab-columbia)]
- **OKAMI**: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.11792)] [[Website](https://ut-austin-rpl.github.io/OKAMI/)]
- **Robots Pre-train Robots**: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.22325)] [[Website](https://robots-pretrain-robots.github.io/)] [[Code](https://github.com/luccachiang/robots-pretrain-robots)]
- **VideoAgent**: Self-Improving Video Generation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.10076)] [[Code](https://github.com/Video-as-Agent/VideoAgent)]
- **IGOR**: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2411.00785)] [[Website](https://www.microsoft.com/en-us/research/project/igor-image-goal-representations/)]
- **VidMan**: Exploiting Implicit Dynamics from Video Diffusion Model for Effective Robot Manipulation, *NeurIPS 2024*. [[Paper](https://arxiv.org/abs/2411.09153)]
- Grounding Video Models to Actions through Goal Conditioned Exploration, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2411.07223)] [[Website](https://video-to-action.github.io/)] [[Code](https://github.com/video-to-action/video-to-action-release)]



## LLM/VLM Guidance

- **SpatialVLM**: Endowing Vision-Language Models with Spatial Reasoning Capabilities, *CVPR 2024*. [[Paper](https://arxiv.org/abs/2401.12168)] [[Website](https://spatial-vlm.github.io/#community-implementation)] [[Unofficial Code](https://github.com/remyxai/VQASynth)]

- **PIVOT**: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs, *arXiv 2024.02*. [[Paper](https://arxiv.org/abs/2402.07872)] [[Website](https://pivot-prompt.github.io/)] [[Demo](https://pivot-prompt.github.io/#demo)]
- 

## Structured Instructions



## Vision-Language-Action

- **RT-1**: Robotics Transformer for Real-World Control at Scale, *arXiv 2022*. [[Paper](https://arxiv.org/abs/2212.06817)] [[Website](https://robotics-transformer1.github.io/)] [[Code](https://github.com/google-research/robotics_transformer)] [[Robotics at Google](https://deepmind.google/discover/blog/)]
- **PaLM-E**: An Embodied Multimodal Language Model, *arXiv 2023*. [[Paper](https://arxiv.org/abs/2303.03378)] [[Website](https://palm-e.github.io/)] [[Robotics at Google](https://deepmind.google/discover/blog/)]
- **VQ-BeT**: Behavior Generation with Latent Actions, *ICML 2024 Spotlight*. [[Paper](https://arxiv.org/abs/2403.03181)] [[Website](https://sjlee.cc/vq-bet/)] [[Code](https://github.com/jayLEE0301/vq_bet_official)]
- **RT-2**: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, *arXiv 2023*. [[Paper](https://arxiv.org/abs/2307.15818)] [[Website](https://robotics-transformer2.github.io/)] [[Unofficial Code](https://github.com/kyegomez/RT-2)] [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **Diffusion Policy**: Visuomotor Policy Learning via Action Diffusion, *RSS 2023*. [[Paper](https://arxiv.org/abs/2303.04137)] [[Website](https://diffusion-policy.cs.columbia.edu/)] [[Code](https://github.com/real-stanford/diffusion_policy)] [[REAL-Stanford](https://github.com/real-stanford)]
- **ALOHA**: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, *RSS 2023*. [[Paper](https://arxiv.org/abs/2304.13705)] [[Code](https://github.com/tonyzhaozh/aloha)] [[Website](https://tonyzhaozh.github.io/aloha/)]
- **ACT**: Action Chunking with Transformers, *RSS 2023*. [[Paper](https://arxiv.org/abs/2304.13705)] [[Code](https://github.com/tonyzhaozh/aloha)] [[Website](https://tonyzhaozh.github.io/aloha/)]
- **LIBERO**: Benchmarking Knowledge Transfer for Lifelong Robot Learning, *NeurIPS 2023*. [[Paper](https://arxiv.org/abs/2306.03310)] [[Website](https://libero-project.github.io/main.html)] [[Code](https://github.com/Lifelong-Robot-Learning/LIBERO)]
- **UniSim**: Learning Interactive Real-World Simulators, *ICLR 2024 (Outstanding Paper Award)*. [[Paper](https://arxiv.org/abs/2310.06114)] [[Website](https://universal-simulator.github.io/unisim/)] [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **ALOHA 2**: An Enhanced Low-Cost Hardware for Bimanual Teleoperation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2405.02292)] [[Code](https://github.com/tonyzhaozh/aloha/tree/main/aloha2)] [[Website](https://aloha-2.github.io/)]  [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **Octo**: An Open-Source Generalist Robot Policy, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2405.12213)] [[Website](https://octo-models.github.io/)] [[Code](https://github.com/octo-models/octo)] [UC Berkeley]
- **HPT**: Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers, *NeurIPS 2024*. [[Paper](https://arxiv.org/abs/2409.20537)] [[Website](https://liruiw.github.io/hpt/)] [[Code](https://github.com/liruiw/HPT)] [[Kaiming He, MIT ](https://people.csail.mit.edu/kaiming/)]
- **RDT-1B**: A Diffusion Foundation Model for Bimanual Manipulation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.07864)] [[Code](https://github.com/thu-ml/RoboticsDiffusionTransformer)] [[Website](https://rdt-robotics.github.io/rdt-robotics/)] [[Jun Zhu, THU](https://scholar.google.com/citations?hl=en&user=axsP38wAAAAJ&view_op=list_works&sortby=pubdate)]
- **GR-1**: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation, *ICLR 2024*. [[Paper](https://arxiv.org/abs/2312.13139)] [[Website](https://gr1-manipulation.github.io/)] [[Code](https://github.com/bytedance/GR-1)] [ByteDance Research]
- **SimplerEnv**: Simulated Manipulation Policy Evaluation Environments for Real Robot Setups, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2405.05941)] [[Website](https://simpler-env.github.io/)] [[Code](https://github.com/simpler-env/SimplerEnv)]
- **Ï€0**: A Vision-Language-Action Flow Model for General Robot Control, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.24164)] [[Website](https://www.physicalintelligence.company/blog/pi0)] [[Physical Intelligence](https://www.physicalintelligence.company/)]
- **Scaling Up and Distilling Down**: Language-Guided Robot Skill Acquisition, *CoRL 2023*. [[Paper](https://openreview.net/forum?id=3uwj8QZROL)] [[Website](https://www.cs.columbia.edu/~huy/scalingup/)] [[Code](https://github.com/real-stanford/scalingup)]
- "Data Scaling Laws in Imitation Learning for Robotic Manipulation", *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.18647)] [[Website](https://data-scaling-laws.github.io/)] [[Code](https://github.com/Fanqi-Lin/Data-Scaling-Laws)] ``8 A100``
- **3D-VLA**: A 3D Vision-Language-Action Generative World Model, *ICML 2024*. [[Paper](https://arxiv.org/abs/2403.09631)] [[Code](https://github.com/UMass-Foundation-Model/3D-VLA)] [[Website](https://vis-www.cs.umass.edu/3dvla/)] [[UMass Foundation Model](https://github.com/UMass-Foundation-Model)]
- A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter, *ICRA 2023*. [[Paper](https://arxiv.org/abs/2302.12610)] [[Code](https://github.com/xukechun/Vision-Language-Grasping)]
- **CogACT**: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2411.19650)] [[Website](https://cogact.github.io/)] [[Code](https://github.com/microsoft/CogACT)]
- **BYOVLA**: Bring Your Own Vision-Language-Action Model, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.01971)] [[Website](https://aasherh.github.io/byovla/)] [[Code](https://github.com/irom-princeton/byovla)]
- **VLMPC**: Vision-Language Model Predictive Control for Robotic Manipulation, *RSS 2024*. [[Paper](https://arxiv.org/abs/2407.09829)] [[Code](https://github.com/PPjmchen/VLMPC)] [[Ran Song, Shandong University](https://faculty.sdu.edu.cn/songran/en/index/1023305/list/index.htm)]
- **3D Diffusion Policy**: Generalizable Visuomotor Policy Learning via Simple 3D Representations, *RSS 2024*. [[Paper](https://arxiv.org/abs/2403.03954)] [[Website](https://3d-diffusion-policy.github.io/)] [[Code](https://github.com/YanjieZe/3D-Diffusion-Policy)]
- **iDP3**: Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.10803)] [[Website](https://humanoid-manipulation.github.io/)] [[Code](https://github.com/YanjieZe/Improved-3D-Diffusion-Policy)]



## Benchmark and Dataset

- **OpenVLA**: An Open-Source Vision-Language-Action Model, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2406.09246)] [[Code](https://github.com/openvla/openvla)] [[Website](https://openvla.github.io/)] [Stanford University] ``64 A100``
- **VLABench**: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, *arXiv 2024.12*. [[Paper](https://arxiv.org/abs/2412.18194)] [[Website](https://vlabench.github.io/)] [[Code](https://github.com/OpenMOSS/VLABench)]
- AgiBot World Colosseum. [[Code](https://github.com/OpenDriveLab/AgiBot-World)] [[Website](https://agibot-world.com/)]