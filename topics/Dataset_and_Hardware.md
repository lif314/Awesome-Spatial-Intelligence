# Dataset and Hardware

## Hardware

### Low-Cost Robot

- **Low-Cost Robot Arm**: [[Code](https://github.com/AlexanderKoch-Koch/low_cost_robot)] [[Company](https://tau-robotics.com/robots)] [[Video](https://youtu.be/RckrXOEoWrk)]

### Open-source Tactile Sensor

- **9DTact**: A Compact Vision-Based Tactile Sensor for Accurate 3D Shape Reconstruction and Generalizable 6D Force Estimation, *RA-L 2023*. [[Paper](https://arxiv.org/abs/2308.14277)] [[Website](https://linchangyi1.github.io/9DTact/)] [[Code](https://github.com/linchangyi1/9DTact)] [[Hardware](https://github.com/linchangyi1/9DTact)] [[Tutorial](https://www.bilibili.com/video/BV1sj42197HJ/?share_source=copy_web)] [[Changyi Lin, CMU](https://linchangyi1.github.io/)]

- **3D-ViTac**: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.24091)] [[Website](https://binghao-huang.github.io/3D-ViTac/)] [[Code comming soon]()] [[Hardware Tutorial](https://docs.google.com/document/d/1XGyn-iV_wzRmcMIsyS3kwcrjxbnvblZAyigwbzDsX-E/edit?tab=t.0#heading=h.ny8zu0pq9mxy)] [[Hardware Code](https://github.com/binghao-huang/3D-ViTac_Tactile_Hardware)]  [[Binghao Huang, Columbia University](https://binghao-huang.github.io/)]
- **GelSlim 4.0**: Focusing on Touch and Reproducibility, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2409.19770)] [[Website](https://www.mmintlab.com/research/gelslim-4-0/)] [[Hardware](https://github.com/MMintLab/gelslim_hardware)] [[Code: gelslim_depth](https://github.com/MMintLab/gelslim_depth)] [[Code: gelslim_shear](https://github.com/MMintLab/gelslim_shear)]

###  Data Collection

- **Universal Manipulation Interface**: In-The-Wild Robot Teaching Without In-The-Wild Robots, *RSS 2024*. [[Paper](https://arxiv.org/abs/2402.10329)] [[Website](https://umi-gripper.github.io/)] [[Code](https://github.com/real-stanford/universal_manipulation_interface)] [[Hardware](https://docs.google.com/document/d/1TPYwV9sNVPAi0ZlAupDMkXZ4CA1hsZx7YDMSmcEy6EU/edit?tab=t.0#heading=h.5k5vwx2iqjqg)] [[real-stanford](https://github.com/real-stanford)]
- **Fast-UMI**: A Scalable and Hardware-Independent Universal Manipulation Interface, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2409.19499)] [[Website](https://fastumi.com/)] [[Data Code](https://github.com/YdingTeam/FastUMI_Data)] [[Yan Ding, Shanghai AI Laboratory](https://yding25.com/)]
- **ALOHA**: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, *RSS 2023*. [[Paper](https://arxiv.org/abs/2304.13705)] [[Website](https://tonyzhaozh.github.io/aloha/)] [[ACT Code](https://github.com/tonyzhaozh/act)] [[ALOHA Code](https://github.com/tonyzhaozh/aloha)] [[Chelsea Finn, Stanford University](https://ai.stanford.edu/~cbfinn/)]
- **ALOHA 2**: An Enhanced Low-Cost Hardware for Bimanual Teleoperation, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2405.02292)] [[Website](https://aloha-2.github.io/)] [[Code](https://github.com/tonyzhaozh/aloha/tree/main/aloha2)] [[Google DeepMind](https://deepmind.google/discover/blog/)]
- **DexCap**: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation, *RSS 2024*. [[Paper](https://arxiv.org/abs/2403.07788)] [[Website](https://dex-cap.github.io/)] [[Code](https://github.com/j96w/DexCap)] [[Hardware](https://docs.google.com/document/d/1ANxSA_PctkqFf3xqAkyktgBgDWEbrFK7b1OnJe54ltw/edit?tab=t.0#heading=h.yxlxo67jgfyx)] [[Li Fei-Fei, Stanford University](https://profiles.stanford.edu/fei-fei-li)]

## Dataset

- [[awesome-robotics-datasets](https://github.com/mint-lab/awesome-robotics-datasets)], 2020.

- **FurnitureBench**: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation, *RSS 2023*. [[Paper](https://arxiv.org/abs/2305.12821)] [[Website](https://clvrai.github.io/furniture-bench/)] [[Code](https://github.com/clvrai/furniture-bench)] [[Docs](https://clvrai.github.io/furniture-bench/docs/index.html)]

- **FMB**: a Functional Manipulation Benchmark for Generalizable Robotic Learning, *IJRR 2024*. [[Paper](https://arxiv.org/abs/2401.08553)] [[Website](https://functional-manipulation-benchmark.github.io/)] [[Code](https://github.com/rail-berkeley/fmb)]

- **DROID**: A Large-Scale In-The-Wild Robot Manipulation Dataset, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2403.12945)] [[Website](https://droid-dataset.github.io/)] [[Code](https://github.com/droid-dataset/droid_policy_learning)] [[Hardware](https://github.com/droid-dataset/droid)]
- **Open X-Embodiment**: Robotic Learning Datasets and RT-X Models, *arXiv 2023*. [[Paper](https://arxiv.org/abs/2310.08864)] [[Website](https://robotics-transformer-x.github.io/)] [[Code](https://github.com/google-deepmind/open_x_embodiment)] [[Dataset](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit#gid=0)] [[Google]]
- 