# Vision Tactile Manipulation



- [Grasping and Manipulation](#Grasping-and-Manipulation)
- [Representation and Perception](#Representation-and-Perception)

## Grasping and Manipulation

- **LeTac-MPC**: Learning Model Predictive Control for Tactile-reactive Grasping, *T-RO 2024*.  [[Paper](https://arxiv.org/abs/2403.04934)] [[Code](https://github.com/ZhengtongXu/LeTac-MPC)] [[Video](https://drive.google.com/file/d/1rDwg7dA3Wfhhb3rhry0cIfAxGli7WT7k/view)] [[Yu She, Purdue University](https://www.purduemars.com/)]
- **Tac-Man**: Tactile-Informed Prior-Free Manipulation of Articulated Objects, *T-RO 2024*. [[Paper](https://arxiv.org/abs/2403.01694)] [[Website](https://tacman-aom.github.io/)] [[Code](https://github.com/YuyangLee/Tac-Man-Simulation)]
- **VITaL Pretraining**: Visuo-Tactile Pretraining for Tactile and Non-Tactile Manipulation Policies, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2403.11898)] [[Website](https://sites.google.com/andrew.cmu.edu/visuo-tactile-pretraining)] [[Code](https://github.com/Abraham190137/TactileACT)]
- **simPLE**: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects, *Science Robotics 2024*. [[Paper](https://arxiv.org/abs/2307.13133)] [[Website](https://mcube.mit.edu/research/simPLE.html)] [[MCube Lab, MIT](https://mcube.mit.edu/publications.html)]
- **RoboPack**: Learning Tactile-Informed Dynamics Models for Dense Packing, *RSS 2024*. [[Paper](https://arxiv.org/abs/2407.01418)] [[Website](https://robo-pack.github.io/)]
- **3D-ViTac**: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.24091)] [[Website](https://binghao-huang.github.io/3D-ViTac/)]

- Visualâ€“Tactile Fusion for Transparent Object Grasping in Complex Backgrounds, *T-RO 2023*. [[Paper](https://arxiv.org/abs/2211.16693)] [[Website](https://sites.google.com/view/visual-tactilefusion)] [[Code](https://github.com/Shoujie1998/Visual-tactile-fusion)]
- **Touch2Touch**: Cross-Modal Tactile Generation for Object Manipulation, *arXiv 2024*. [[Paper](https://www.arxiv.org/abs/2409.08269)] [[Website](https://www.mmintlab.com/research/touch2touch/)] [[Code](https://github.com/MMintLab/touch2touch)]



---

## Representation and Perception

- **TaRF**: Tactile-Augmented Radiance Fields, *CVPR 2024*. [[Paper](https://arxiv.org/abs/2405.04534)] [[Website](https://www.yimingdou.com/TaRF/)] [[Code](https://github.com/Dou-Yiming/TaRF)]

- **Tactile DreamFusion**: Exploiting Tactile Sensing for 3D Generation, *NeurIPS 2024*. [[Paper](https://arxiv.org/abs/2412.06785)] [[Website](https://ruihangao.github.io/TactileDreamFusion/)] [[Code](https://github.com/RuihanGao/TactileDreamFusion)]
- **Binding Touch to Everything**: Learning Unified Multimodal Tactile Representations, *CVPR 2024*. [[Paper](https://arxiv.org/abs/2401.18084)] [[Website](https://cfeng16.github.io/UniTouch/)] [[Code](https://github.com/cfeng16/UniTouch)]
- **TVL**: A Touch, Vision, and Language Dataset for Multimodal Alignment, *ICML 2024 (Oral)*. [[Paper](https://arxiv.org/abs/2401.14391)] [[Website](https://tactile-vlm.github.io/)] [[Code](https://github.com/Max-Fu/tvl)]
- **CTTP**: Contrastive Touch-to-Touch Pretraining, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.11834)] [[Website](https://www.mmintlab.com/research/cttp/)] [[Code](https://github.com/MMintLab/CTTP)]

- **T3**: Transferable Tactile Transformers for Representation Learning Across Diverse Sensors and Tasks, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2406.13640)] [[Website](https://t3.alanz.info/)] [[Code](https://github.com/alanzjl/t3)]
- **UniT**: Unified Tactile Representation for Robot Learning, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2408.06481)] [[Website](https://zhengtongxu.github.io/unifiedtactile.github.io/)] [[Code](https://github.com/ZhengtongXu/UniT)]  [[Yu She, Purdue University](https://www.purduemars.com/)]

- :fire: **Sparsh**: Self-supervised touch representations for vision-based tactile sensing, *CoRL 2024*. [[Paper](https://arxiv.org/abs/2410.24090)] [[Website](https://sparsh-ssl.github.io/)] [[Code](https://github.com/facebookresearch/sparsh)] [AI at Meta, FAIR](https://ai.facebook.com/research/ai-systems)

- **FeelAnyForce**: Estimating Contact Force Feedback from Tactile Sensation for Vision-Based Tactile Sensors, *arXiv 2024*. [[Paper](https://arxiv.org/abs/2410.02048)] [[Website](https://prg.cs.umd.edu/FeelAnyForce)]
- **iFEM2.0**: Dense 3D Contact Force Field Reconstruction and Assessment for Vision-Based Tactile Sensors, *TRO 2024*.  [[Paper](https://ieeexplore.ieee.org/abstract/document/10758225)]
- :fire: **Octopi**: Object Property Reasoning with Large Tactile-Language Models, *RSS 2024*. [[Paper](https://arxiv.org/abs/2405.02794)] [[Code](https://github.com/clear-nus/octopi)]
- **NeuralFeels with neural fields**: Visuotactile perception for in-hand manipulation, *Science Robotics 2024*. [[Paper](https://arxiv.org/abs/2312.13469)] [[Website](https://suddhu.github.io/neural-feels/)] [[Code](https://github.com/facebookresearch/neuralfeels)] [[CMU](https://www.ri.cmu.edu/)]
- **SITR**: Sensor-Invariant Tactile Representation, *ICLR 2025*. [[Paper](https://arxiv.org/abs/2502.19638)] [[Website](https://hgupt3.github.io/sitr/)] [[Code](https://github.com/hgupt3/gsrl)] [[Dataset](https://huggingface.co/datasets/hgupt3/sitr_dataset)]
- **AnyTouch**: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors, *ICLR 2025*. [[Paper](https://arxiv.org/abs/2502.12191)] [[Website](https://gewu-lab.github.io/AnyTouch/)] [[Code](https://github.com/GeWu-Lab/AnyTouch)] [[Dataset](https://huggingface.co/datasets/xxuan01/TacQuad)]
- **Touch100k**: A Large-Scale Touch-Language-Vision Dataset for Touch-Centric Multimodal Representation, *arXiv 2025*. [[Paper](https://arxiv.org/abs/2406.03813)] [[Website](https://cocacola-lab.github.io/Touch100k/)] [[Code](https://github.com/cocacola-lab/TLV-Link)]